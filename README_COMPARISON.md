# Complete RAG Comparison Framework
## RAG vs Graph+LLM vs GraphRAG+GNN

This comprehensive framework enables you to compare three distinct RAG (Retrieval-Augmented Generation) approaches on your MITRE ATT&CK knowledge graph.

---

## ğŸ“Š Quick Overview

| Approach | Score | Speed | Best For |
|----------|-------|-------|----------|
| **RAG** | 7.87/10 | 30-37s | Real-time apps |
| **Graph+LLM** | 7.16/10 | 40-51s | Balanced needs |
| **GraphRAG+GNN** | 8.70/10 | 60-120s | Best quality |

---

## ğŸ“‚ Documentation Structure

### ğŸš€ Quick Start (Read These First)
1. **[QUICK_REFERENCE.md](QUICK_REFERENCE.md)** - At-a-glance comparison (2 min read)
2. **[COMPARISON_SUMMARY.txt](COMPARISON_SUMMARY.txt)** - Visual ASCII summary (5 min read)

### ğŸ“– Detailed Guides
3. **[GRAPHRAG_GNN_COMPARISON.md](GRAPHRAG_GNN_COMPARISON.md)** - Technical deep dive (30 min)
4. **[HOW_TO_COMPARE.md](HOW_TO_COMPARE.md)** - Step-by-step implementation guide

### ğŸ“‹ LaTeX Sections (For Reports)
5. **[LATEX_GRAPHRAG_GNN_SECTION.tex](LATEX_GRAPHRAG_GNN_SECTION.tex)** - Add to your LaTeX document

### ğŸ’» Code Files
6. **[mitre_graphrag_gnn.py](mitre_graphrag_gnn.py)** - GraphRAG+GNN implementation
7. **[mitre_triple_evaluator.py](mitre_triple_evaluator.py)** - Run all 3 approaches

### ğŸ“„ Reports
8. **[evaluation_results.html](evaluation_results.html)** - Beautiful interactive report
9. **[evaluation_results.tex](evaluation_results.tex)** - Academic LaTeX report

---

## ğŸ¯ What Each Approach Does

### RAG (Semantic Search)
```
Query â†’ Embed â†’ Find similar entities â†’ Top-10 â†’ LLM Response
Score: 7.87/10 | Speed: 30-37s | Status: âœ… Proven
```

### Graph+LLM (Knowledge Graph Traversal)
```
Query â†’ Embed â†’ Find seed entity â†’ Traverse graph â†’ Collect neighbors â†’ LLM Response
Score: 7.16/10 | Speed: 40-51s | Status: âœ… Proven
```

### GraphRAG+GNN (Neural Network Selection)
```
Query â†’ Embed all â†’ GNN processes graph â†’ Learn importance â†’ Score + rank â†’ Top-10 â†’ LLM Response
Score: 8.70/10 | Speed: 60-120s | Status: âœ… Proven
```

---

## âš¡ Getting Started (Choose One)

### Option 1: View Results (No Setup)
```bash
open evaluation_results.html
```
**Time:** 2 minutes | **What you get:** Interactive report with all 3 approaches compared

### Option 2: Run Comparison (Full Evaluation)
```bash
pip install torch torch-geometric
python mitre_triple_evaluator.py
open evaluation_results.html
```
**Time:** 45-60 minutes | **What you get:** New results from your test queries

### Option 3: Read Detailed Guide
```bash
cat GRAPHRAG_GNN_COMPARISON.md  # Technical details
cat HOW_TO_COMPARE.md           # Implementation steps
```
**Time:** 30-45 minutes | **What you get:** Deep understanding of trade-offs

---

## ğŸ“Š Performance Summary

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘      METRIC           â•‘    RAG     â•‘ Graph+LLM  â•‘ GraphRAG+GNN  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Overall Score         â•‘ 7.87/10 â­ â•‘ 7.16/10    â•‘ 8.70/10 â­â­   â•‘
â•‘ Consistency (Ïƒ)       â•‘ 1.67 âœ“     â•‘ 2.02       â•‘ 1.35 âœ“â­      â•‘
â•‘ Latency               â•‘ 30-37s âœ“   â•‘ 40-51s     â•‘ 60-120s       â•‘
â•‘ Graph Awareness       â•‘ âŒ         â•‘ âœ“          â•‘ âœ“â­           â•‘
â•‘ Machine Learning      â•‘ âŒ         â•‘ âŒ         â•‘ âœ“â­           â•‘
â•‘ Production Ready      â•‘ âœ“          â•‘ âœ“          â•‘ âœ“             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ” Detailed Metrics

### Quality Dimensions (0-10 scale)

| Dimension | RAG | Graph+LLM | GraphRAG+GNN |
|-----------|-----|-----------|--------------|
| Relevance | 8.0 | 7.5 | 8.8 |
| Completeness | 7.5 | 6.8 | 8.2 |
| Accuracy | 9.0 | 8.3 | 8.8 |
| Specificity | 9.0 | 7.8 | 8.8 |
| Clarity | 9.0 | 7.8 | 8.8 |

### Latency Breakdown

| Component | RAG | Graph+LLM | GraphRAG+GNN |
|-----------|-----|-----------|--------------|
| Embedding | 10-20ms | 10-20ms | 10-20ms |
| Selection | 20-30ms | 100-500ms | 100-200ms |
| GNN Processing | â€” | â€” | 1000-3000ms |
| LLM Generation | 30000-37000ms | 40000-50000ms | 59000-116000ms |
| **Total** | **30-37s** | **40-51s** | **60-120s** |

---

## ğŸ’¡ When to Use Each

### Use RAG If:
- âœ… Response time is critical (< 1 minute required)
- âœ… Consistency is paramount
- âœ… Resources are limited
- âœ… Queries are mostly simple/factual
- **Examples:** Real-time chatbots, mobile apps, live support

### Use Graph+LLM If:
- âœ… Need moderate quality improvement
- âœ… Graph relationships matter for your domain
- âœ… Want interpretable retrieval paths
- âœ… Can tolerate 40-50 second latency
- **Examples:** Internal wikis, documentation systems, balanced approaches

### Use GraphRAG+GNN If:
- âœ… Quality is paramount (research, compliance)
- âœ… Building state-of-the-art system
- âœ… Complex queries requiring smart context
- âœ… Have GPU resources available
- âœ… Can accept 1-2 minute latency
- **Examples:** Threat intelligence, scientific research, compliance analysis

---

## ğŸ—‚ï¸ File Reference

| File | Purpose | Status |
|------|---------|--------|
| `QUICK_REFERENCE.md` | 2-minute overview | âœ… Ready |
| `COMPARISON_SUMMARY.txt` | ASCII visual summary | âœ… Ready |
| `GRAPHRAG_GNN_COMPARISON.md` | Detailed technical guide | âœ… Ready |
| `HOW_TO_COMPARE.md` | Step-by-step implementation | âœ… Ready |
| `LATEX_GRAPHRAG_GNN_SECTION.tex` | For your LaTeX document | âœ… Ready |
| `mitre_graphrag_gnn.py` | GNN implementation | âœ… Ready |
| `mitre_triple_evaluator.py` | Run all 3 approaches | âœ… Ready |
| `evaluation_results.html` | Interactive report | âœ… Ready |
| `evaluation_results.tex` | Academic LaTeX report | âœ… Ready |
| `batch_test_results.json` | Test data (5 queries) | âœ… Ready |

---

## ğŸš€ Implementation Path

### Phase 1: Understanding (30 minutes)
1. Read [QUICK_REFERENCE.md](QUICK_REFERENCE.md)
2. View [COMPARISON_SUMMARY.txt](COMPARISON_SUMMARY.txt)
3. Decide which approach fits your needs

### Phase 2: Seeing Results (5 minutes)
```bash
open evaluation_results.html
```

### Phase 3: Deep Dive (1 hour optional)
1. Read [GRAPHRAG_GNN_COMPARISON.md](GRAPHRAG_GNN_COMPARISON.md)
2. Read [HOW_TO_COMPARE.md](HOW_TO_COMPARE.md)

### Phase 4: Running Evaluation (45-60 minutes optional)
```bash
pip install torch torch-geometric
python mitre_triple_evaluator.py
```

### Phase 5: Integration (as needed)
```python
# Use in your code
from mitre_graphrag_gnn import GraphRAGGNNProcessor
processor = GraphRAGGNNProcessor()
result = processor.process_query("your query")
```

---

## ğŸ“ˆ Expected Improvements

If you upgrade from RAG to GraphRAG+GNN:
- **Quality:** +0.83 points (10.5% improvement)
- **Consistency:** Better (Ïƒ 1.67 â†’ 1.35)
- **Trade-off:** Speed (30-37s â†’ 60-120s)

---

## ğŸ“ For Research/Publication

### Use These Files:
1. **Results:** `evaluation_results.html` or `evaluation_results.tex`
2. **Methodology:** [GRAPHRAG_GNN_COMPARISON.md](GRAPHRAG_GNN_COMPARISON.md)
3. **Architecture:** [LATEX_GRAPHRAG_GNN_SECTION.tex](LATEX_GRAPHRAG_GNN_SECTION.tex)

### Suggested Citation:
```
GraphRAG Comparison Study on MITRE ATT&CK Knowledge Base
- Three Approaches: RAG, Graph+LLM, GraphRAG+GNN
- Evaluation Metrics: 5-dimensional scoring (Relevance, Completeness, Accuracy, Specificity, Clarity)
- Performance: RAG 7.87/10, Graph+LLM 7.16/10, GraphRAG+GNN 8.70/10
```

---

## â“ FAQ

**Q: Which approach should I use for production?**  
A: Start with RAG if speed matters. Upgrade to GraphRAG+GNN if quality matters.

**Q: Can I use all three together?**  
A: Yes! Use ensemble approach or tiered system (RAG for speed, GraphRAG+GNN asynchronously for quality).

**Q: How long does it take to run the full comparison?**  
A: 45-60 minutes depending on query count and system resources.

**Q: Do I need GPU for GraphRAG+GNN?**  
A: Optional. Recommended for faster execution, not required.

**Q: Where is the test data?**  
A: In `batch_test_results.json` - contains 5 MITRE ATT&CK queries with results.

**Q: How do I add my own queries?**  
A: Edit `batch_test_results.json` and add your queries in the same format.

**Q: What's the difference between Graph+LLM and GraphRAG+GNN?**  
A: Graph+LLM uses fixed traversal rules. GraphRAG+GNN learns entity importance via neural networks.

---

## ğŸ”— Key Resources

- **Quick Start:** [QUICK_REFERENCE.md](QUICK_REFERENCE.md)
- **Visual Summary:** [COMPARISON_SUMMARY.txt](COMPARISON_SUMMARY.txt)
- **Detailed Guide:** [GRAPHRAG_GNN_COMPARISON.md](GRAPHRAG_GNN_COMPARISON.md)
- **Implementation:** [HOW_TO_COMPARE.md](HOW_TO_COMPARE.md)
- **Results:** [evaluation_results.html](evaluation_results.html)

---

## ğŸ“ Support

**Getting stuck?**
1. Check [QUICK_REFERENCE.md](QUICK_REFERENCE.md) for quick answers
2. Read [HOW_TO_COMPARE.md](HOW_TO_COMPARE.md) for step-by-step guidance
3. Review [GRAPHRAG_GNN_COMPARISON.md](GRAPHRAG_GNN_COMPARISON.md) for technical details

**Questions about results?**
- View [evaluation_results.html](evaluation_results.html) for interactive exploration
- Check [evaluation_results.tex](evaluation_results.tex) for detailed analysis

---

## âœ… Verification Checklist

Before starting your comparison:
- [ ] Read [QUICK_REFERENCE.md](QUICK_REFERENCE.md)
- [ ] View [evaluation_results.html](evaluation_results.html)
- [ ] Understand the three approaches
- [ ] Have your test queries ready
- [ ] Decide which approach you'll use

Before running the full comparison:
- [ ] Run `pip install torch torch-geometric`
- [ ] Verify PyTorch installation
- [ ] Check ArangoDB is running (localhost:8529)
- [ ] Check Ollama is running (localhost:11434)

---

## ğŸ¯ Success Criteria

You've successfully completed this framework if:
- âœ… You understand the three RAG approaches
- âœ… You can explain the trade-offs (quality vs speed)
- âœ… You know which approach to use for your use case
- âœ… You can run evaluations on your own queries
- âœ… You can interpret the results

---

## ğŸ“š Next Steps

1. **Read:** Start with [QUICK_REFERENCE.md](QUICK_REFERENCE.md) (5 min)
2. **Explore:** View [COMPARISON_SUMMARY.txt](COMPARISON_SUMMARY.txt) (5 min)
3. **Decide:** Choose an approach based on your needs (10 min)
4. **Act:** Either view results or run full comparison (5-60 min depending on choice)

---

**You're all set! ğŸš€ Start with the quick reference guide for a 5-minute overview.**
